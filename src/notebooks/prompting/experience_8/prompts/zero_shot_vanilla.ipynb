{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.sys.path.append(os.path.join(os.getcwd(), \"../../../..\"))\n",
    "from models.Prompt import PromptRunParameters\n",
    "from prompting.run_prompt import WrapperEnum, run_prompts\n",
    "from prompting.get_prompts import zero_shot_vanilla\n",
    "from prompting.get_themes_list import selected_level_1_themes_first_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "! ONLY EDIT THIS CELL\n",
    "\"\"\"\n",
    "\n",
    "#? Defines the themes list and its associated hierarchy level (0 to 3)\n",
    "THEMES_LIST, THEMES_HIERARCHY_LEVEL = selected_level_1_themes_first_version()\n",
    "THEMES_LIST.append(\"numérique\")\n",
    "#? Defines the list of prompts to use, its associated prompt type (zero-shot, one-shot, etc.)\n",
    "PROMPTS, ASSOC, ACCEPTED_THEMES_FOR_QUESTION, PROMPT_TYPES = zero_shot_vanilla(THEMES_LIST, THEMES_HIERARCHY_LEVEL)\n",
    "\n",
    "#? Specify the API wrapper to use for calling the LLM\n",
    "WRAPPER = WrapperEnum.OpenAI\n",
    "#? Define the model to use with the wrapper (available models can be found in the PROMPTING.md file)\n",
    "MODEL = \"gpt-4o-mini-2024-07-18\"\n",
    "#? Set the temperature parameter for the model to control randomness (0.0 = deterministic, 1.0 = random)\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "#? The identifier for the batch of questions being processed. Set to None if no Batch should be used\n",
    "BATCH_ID = \"670ee26140ec76caf27bd038\"\n",
    "#? Number of questions sampled from the database. Will create a Batch based on the themes list\n",
    "QUESTION_SAMPLE_SIZE = 1500\n",
    "\n",
    "# ? Name for the run\n",
    "NAME = \"zero shot w/ numérique\"\n",
    "#? Add a comment to be able to identify each run\n",
    "DESCRIPTION = \"experience #8 zero shot vanilla w/ numérique\"\n",
    "#? If the API calls starts to be slower, increase the sleep time\n",
    "SLEEP_TIME=0\n",
    "#? Number of run to create with the same parameters\n",
    "NUMBER_OF_RUNS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "! DO NOT EDIT THIS CELL\n",
    "\"\"\"\n",
    "PARAMETERS = PromptRunParameters(\n",
    "    temperature=TEMPERATURE,\n",
    "    model=MODEL,\n",
    "    types=PROMPT_TYPES,\n",
    "    theme_hierarchy_level=THEMES_HIERARCHY_LEVEL,\n",
    "    wrapper=WRAPPER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "! DO NOT EDIT THIS CELL\n",
    "\"\"\"\n",
    "for i in range(1, NUMBER_OF_RUNS + 1):\n",
    "\trun_prompts(\n",
    "\t\tparameters=PARAMETERS,\n",
    "\t\tprompts=PROMPTS,\n",
    "\t\tthemes_list=THEMES_LIST,\n",
    "\t\tnumber_of_questions=QUESTION_SAMPLE_SIZE,\n",
    "\t\tbatch_id=BATCH_ID,\n",
    "\t\taccepted_themes_for_questions=ACCEPTED_THEMES_FOR_QUESTION,\n",
    "\t\tdescription=DESCRIPTION,\n",
    "\t\tname=NAME,\n",
    "\t\tsleep_time=SLEEP_TIME\n",
    "\t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
