{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "os.sys.path.append(os.path.join(os.getcwd(), \"../../../..\"))\n",
        "from models.Prompt import PromptRunParameters\n",
        "from prompting.run_prompt import WrapperEnum, run_prompts\n",
        "from prompting.get_prompts import zero_shot_assistant_role\n",
        "from prompting.get_themes_list import selected_level_1_themes_first_version\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "! ONLY EDIT THIS CELL\n",
        "\"\"\"\n",
        "\n",
        "#? Defines the themes list and its associated hierarchy level (0 to 3)\n",
        "THEMES_LIST, THEMES_HIERARCHY_LEVEL = selected_level_1_themes_first_version()\n",
        "#? Defines the list of prompts to use, its associated prompt type (zero-shot, one-shot, etc.)\n",
        "PROMPTS, ASSOC, ACCEPTED_THEMES_FOR_QUESTION, PROMPT_TYPES = zero_shot_assistant_role(THEMES_LIST, THEMES_HIERARCHY_LEVEL)\n",
        "\n",
        "#? Specify the API wrapper to use for calling the LLM\n",
        "WRAPPER = WrapperEnum.OpenAI\n",
        "#? Define the model to use with the wrapper (available models can be found in the PROMPTING.md file)\n",
        "MODEL = \"gpt-4o-mini-2024-07-18\"\n",
        "#? Set the temperature parameter for the model to control randomness (0.0 = deterministic, 1.0 = random)\n",
        "TEMPERATURE = 0.0\n",
        "\n",
        "#? The identifier for the batch of questions being processed. Set to None if no Batch should be used\n",
        "BATCH_ID = \"66f9ca5824e5760146e27835\"\n",
        "#? Number of questions sampled from the database. Will create a Batch based on the themes list\n",
        "QUESTION_SAMPLE_SIZE = 1500\n",
        "\n",
        "# ? Name for the run\n",
        "NAME = \"zero shot assistant role\"\n",
        "#? Add a comment to be able to identify each run\n",
        "DESCRIPTION = \"experience #1 zero shot assistant role\"\n",
        "#? If the API calls starts to be slower, increase the sleep time\n",
        "SLEEP_TIME=0.0\n",
        "#? Number of run to create with the same parameters\n",
        "NUMBER_OF_RUNS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "! DO NOT EDIT THIS CELL\n",
        "\"\"\"\n",
        "\n",
        "PARAMETERS = PromptRunParameters(\n",
        "    temperature=TEMPERATURE,\n",
        "    model=MODEL,\n",
        "    types=PROMPT_TYPES,\n",
        "    theme_hierarchy_level=THEMES_HIERARCHY_LEVEL,\n",
        "    wrapper=WRAPPER\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "! DO NOT EDIT THIS CELL\n",
        "\"\"\"\n",
        "\n",
        "for i in range(1, NUMBER_OF_RUNS + 1):\n",
        "\trun_prompts(\n",
        "\t\tparameters=PARAMETERS,\n",
        "\t\tprompts=PROMPTS,\n",
        "\t\tthemes_list=THEMES_LIST,\n",
        "\t\tnumber_of_questions=QUESTION_SAMPLE_SIZE,\n",
        "\t\tbatch_id=BATCH_ID,\n",
        "\t\taccepted_themes_for_questions=ACCEPTED_THEMES_FOR_QUESTION,\n",
        "\t\tdescription=DESCRIPTION,\n",
        "\t\tname=NAME,\n",
        "\t\tsleep_time=SLEEP_TIME\n",
        "\t)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
